{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJnBD6qvVLcpPzhohLXRQs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rianachatterjee04/GenAssist/blob/main/Phoenix_Neighborhood_Walk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing dependencies"
      ],
      "metadata": {
        "id": "wBbZqwgmikth"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bz8EtfJCiUWF",
        "outputId": "39b6dfc6-f7cb-405c-97ea-ac6840557a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.75)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.1.26)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "\n",
        "!pip install yt-dlp\n",
        "\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Video Download"
      ],
      "metadata": {
        "id": "85oqxLAmi6Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Create videos directory\n",
        "os.makedirs(\"videos\", exist_ok=True)\n",
        "\n",
        "# Download video\n",
        "VIDEO_URL = \"https://www.youtube.com/watch?v=bZnjrRuBT_k\"\n",
        "!yt-dlp -o \"videos/Phoenix_Walk.%(ext)s\" {VIDEO_URL}  # Note the capital 'W'\n",
        "\n",
        "# Find the downloaded video file - match the exact case\n",
        "video_path = glob.glob(\"videos/Phoenix_Walk.*\")[0]  # Changed to match case\n",
        "print(f\"Video downloaded to: {video_path}\")\n",
        "\n",
        "# Get absolute path\n",
        "abs_path = os.path.abspath(video_path)\n",
        "print(f\"Absolute path: {abs_path}\")\n",
        "\n",
        "# Verify file exists\n",
        "if os.path.exists(video_path):\n",
        "    print(\"✅ Video file exists!\")\n",
        "    print(f\"File size: {os.path.getsize(video_path) / (1024*1024):.2f} MB\")\n",
        "else:\n",
        "    print(\"❌ Video file NOT found. Check the file name and location.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dNXa1L1i8NZ",
        "outputId": "4c279546-883b-4f22-91f9-88a85c42cedb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=bZnjrRuBT_k\n",
            "[youtube] bZnjrRuBT_k: Downloading webpage\n",
            "[youtube] bZnjrRuBT_k: Downloading tv client config\n",
            "[youtube] bZnjrRuBT_k: Downloading player 9c6dfc4a\n",
            "[youtube] bZnjrRuBT_k: Downloading tv player API JSON\n",
            "[youtube] bZnjrRuBT_k: Downloading ios player API JSON\n",
            "[youtube] bZnjrRuBT_k: Downloading m3u8 information\n",
            "[info] bZnjrRuBT_k: Downloading 1 format(s): 315+328\n",
            "[download] Destination: videos/Phoenix_Walk.f315.webm\n",
            "\u001b[K[download] 100% of    1.76GiB in \u001b[1;37m00:01:39\u001b[0m at \u001b[0;32m18.01MiB/s\u001b[0m\n",
            "[download] Destination: videos/Phoenix_Walk.f328.m4a\n",
            "\u001b[K[download] 100% of   28.67MiB in \u001b[1;37m00:00:01\u001b[0m at \u001b[0;32m19.33MiB/s\u001b[0m\n",
            "[Merger] Merging formats into \"videos/Phoenix_Walk.mkv\"\n",
            "Deleting original file videos/Phoenix_Walk.f328.m4a (pass -k to keep)\n",
            "Deleting original file videos/Phoenix_Walk.f315.webm (pass -k to keep)\n",
            "Video downloaded to: videos/Phoenix_Walk.mkv\n",
            "Absolute path: /content/videos/Phoenix_Walk.mkv\n",
            "✅ Video file exists!\n",
            "File size: 1826.83 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting frames for analysis"
      ],
      "metadata": {
        "id": "agZawzcKi-GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Find the downloaded video file\n",
        "video_path = glob.glob(\"videos/Phoenix_Walk.*\")[0]  # Use the same path from first script\n",
        "output_dir = \"frames\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Open video file\n",
        "vidcap = cv2.VideoCapture(video_path)\n",
        "if not vidcap.isOpened():\n",
        "    print(\"❌ Error: Could not open video.\")\n",
        "    exit()\n",
        "\n",
        "# Get video properties\n",
        "fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
        "total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "duration = total_frames / fps\n",
        "print(f\"Video FPS: {fps}\")\n",
        "print(f\"Total Frames: {total_frames}\")\n",
        "print(f\"Duration: {duration:.2f} seconds\")\n",
        "\n",
        "# Extract frames from 83s to 133s\n",
        "start_sec, end_sec = 86, 133\n",
        "start_frame = start_sec * fps\n",
        "end_frame = min(end_sec * fps, total_frames)\n",
        "\n",
        "vidcap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "frame_rate = 30\n",
        "count = 0\n",
        "\n",
        "while vidcap.isOpened():\n",
        "    frame_id = int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "    if frame_id > end_frame:\n",
        "        break\n",
        "\n",
        "    success, image = vidcap.read()\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    if frame_id % frame_rate == 0:\n",
        "        frame_name = os.path.join(output_dir, f\"frame_{frame_id}.jpg\")\n",
        "        cv2.imwrite(frame_name, image)\n",
        "        print(f\"✅ Saved: {frame_name}\")\n",
        "        count += 1\n",
        "\n",
        "vidcap.release()\n",
        "print(f\"✅ Extracted {count} frames\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR2sJgU1jBp3",
        "outputId": "867fdef0-ed38-497d-8e8c-a4995483e978",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video FPS: 60\n",
            "Total Frames: 37563\n",
            "Duration: 626.05 seconds\n",
            "✅ Saved: frames/frame_5160.jpg\n",
            "✅ Saved: frames/frame_5190.jpg\n",
            "✅ Saved: frames/frame_5220.jpg\n",
            "✅ Saved: frames/frame_5250.jpg\n",
            "✅ Saved: frames/frame_5280.jpg\n",
            "✅ Saved: frames/frame_5310.jpg\n",
            "✅ Saved: frames/frame_5340.jpg\n",
            "✅ Saved: frames/frame_5370.jpg\n",
            "✅ Saved: frames/frame_5400.jpg\n",
            "✅ Saved: frames/frame_5430.jpg\n",
            "✅ Saved: frames/frame_5460.jpg\n",
            "✅ Saved: frames/frame_5490.jpg\n",
            "✅ Saved: frames/frame_5520.jpg\n",
            "✅ Saved: frames/frame_5550.jpg\n",
            "✅ Saved: frames/frame_5580.jpg\n",
            "✅ Saved: frames/frame_5610.jpg\n",
            "✅ Saved: frames/frame_5640.jpg\n",
            "✅ Saved: frames/frame_5670.jpg\n",
            "✅ Saved: frames/frame_5700.jpg\n",
            "✅ Saved: frames/frame_5730.jpg\n",
            "✅ Saved: frames/frame_5760.jpg\n",
            "✅ Saved: frames/frame_5790.jpg\n",
            "✅ Saved: frames/frame_5820.jpg\n",
            "✅ Saved: frames/frame_5850.jpg\n",
            "✅ Saved: frames/frame_5880.jpg\n",
            "✅ Saved: frames/frame_5910.jpg\n",
            "✅ Saved: frames/frame_5940.jpg\n",
            "✅ Saved: frames/frame_5970.jpg\n",
            "✅ Saved: frames/frame_6000.jpg\n",
            "✅ Saved: frames/frame_6030.jpg\n",
            "✅ Saved: frames/frame_6060.jpg\n",
            "✅ Saved: frames/frame_6090.jpg\n",
            "✅ Saved: frames/frame_6120.jpg\n",
            "✅ Saved: frames/frame_6150.jpg\n",
            "✅ Saved: frames/frame_6180.jpg\n",
            "✅ Saved: frames/frame_6210.jpg\n",
            "✅ Saved: frames/frame_6240.jpg\n",
            "✅ Saved: frames/frame_6270.jpg\n",
            "✅ Saved: frames/frame_6300.jpg\n",
            "✅ Saved: frames/frame_6330.jpg\n",
            "✅ Saved: frames/frame_6360.jpg\n",
            "✅ Saved: frames/frame_6390.jpg\n",
            "✅ Saved: frames/frame_6420.jpg\n",
            "✅ Saved: frames/frame_6450.jpg\n",
            "✅ Saved: frames/frame_6480.jpg\n",
            "✅ Saved: frames/frame_6510.jpg\n",
            "✅ Saved: frames/frame_6540.jpg\n",
            "✅ Saved: frames/frame_6570.jpg\n",
            "✅ Saved: frames/frame_6600.jpg\n",
            "✅ Saved: frames/frame_6630.jpg\n",
            "✅ Saved: frames/frame_6660.jpg\n",
            "✅ Saved: frames/frame_6690.jpg\n",
            "✅ Saved: frames/frame_6720.jpg\n",
            "✅ Saved: frames/frame_6750.jpg\n",
            "✅ Saved: frames/frame_6780.jpg\n",
            "✅ Saved: frames/frame_6810.jpg\n",
            "✅ Saved: frames/frame_6840.jpg\n",
            "✅ Saved: frames/frame_6870.jpg\n",
            "✅ Saved: frames/frame_6900.jpg\n",
            "✅ Saved: frames/frame_6930.jpg\n",
            "✅ Saved: frames/frame_6960.jpg\n",
            "✅ Saved: frames/frame_6990.jpg\n",
            "✅ Saved: frames/frame_7020.jpg\n",
            "✅ Saved: frames/frame_7050.jpg\n",
            "✅ Saved: frames/frame_7080.jpg\n",
            "✅ Saved: frames/frame_7110.jpg\n",
            "✅ Saved: frames/frame_7140.jpg\n",
            "✅ Saved: frames/frame_7170.jpg\n",
            "✅ Saved: frames/frame_7200.jpg\n",
            "✅ Saved: frames/frame_7230.jpg\n",
            "✅ Saved: frames/frame_7260.jpg\n",
            "✅ Saved: frames/frame_7290.jpg\n",
            "✅ Saved: frames/frame_7320.jpg\n",
            "✅ Saved: frames/frame_7350.jpg\n",
            "✅ Saved: frames/frame_7380.jpg\n",
            "✅ Saved: frames/frame_7410.jpg\n",
            "✅ Saved: frames/frame_7440.jpg\n",
            "✅ Saved: frames/frame_7470.jpg\n",
            "✅ Saved: frames/frame_7500.jpg\n",
            "✅ Saved: frames/frame_7530.jpg\n",
            "✅ Saved: frames/frame_7560.jpg\n",
            "✅ Saved: frames/frame_7590.jpg\n",
            "✅ Saved: frames/frame_7620.jpg\n",
            "✅ Saved: frames/frame_7650.jpg\n",
            "✅ Saved: frames/frame_7680.jpg\n",
            "✅ Saved: frames/frame_7710.jpg\n",
            "✅ Saved: frames/frame_7740.jpg\n",
            "✅ Saved: frames/frame_7770.jpg\n",
            "✅ Saved: frames/frame_7800.jpg\n",
            "✅ Saved: frames/frame_7830.jpg\n",
            "✅ Saved: frames/frame_7860.jpg\n",
            "✅ Saved: frames/frame_7890.jpg\n",
            "✅ Saved: frames/frame_7920.jpg\n",
            "✅ Saved: frames/frame_7950.jpg\n",
            "✅ Saved: frames/frame_7980.jpg\n",
            "✅ Extracted 95 frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Safety Zone Detector"
      ],
      "metadata": {
        "id": "E2XaKpvQjEI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.utils.plotting import Annotator, colors\n",
        "\n",
        "class SafetyZoneDetector:\n",
        "    def __init__(self):\n",
        "        self.model = YOLO(\"yolov8n-seg.pt\")\n",
        "        self.safety_classes = {\n",
        "            'sidewalk': {'type': 'safe', 'color': (0, 255, 0), 'alpha': 0.3},\n",
        "            'road': {'type': 'danger', 'color': (0, 0, 255), 'alpha': 0.3},\n",
        "            'person': {'type': 'dynamic', 'color': (255, 165, 0), 'alpha': 0.5},\n",
        "            'car': {'type': 'danger', 'color': (255, 0, 0), 'alpha': 0.5},\n",
        "            'bicycle': {'type': 'dynamic', 'color': (255, 165, 0), 'alpha': 0.5},\n",
        "            'traffic light': {'type': 'guide', 'color': (255, 255, 0), 'alpha': 0.5}\n",
        "        }\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        height, width = frame.shape[:2]\n",
        "        safety_overlay = np.zeros_like(frame)\n",
        "        object_overlay = np.zeros_like(frame)\n",
        "        processed = frame.copy()\n",
        "\n",
        "        # Run model inference with tracking\n",
        "        results = self.model.track(frame, persist=True)[0]\n",
        "\n",
        "        if results.boxes is not None and results.masks is not None:\n",
        "            try:\n",
        "                boxes = results.boxes.xyxy.cpu().numpy()\n",
        "                classes = results.boxes.cls.cpu().numpy()\n",
        "                masks = results.masks.xy\n",
        "                track_ids = results.boxes.id.cpu().numpy() if results.boxes.id is not None else None\n",
        "\n",
        "                for i, (box, cls) in enumerate(zip(boxes, classes)):\n",
        "                    class_name = self.model.names[int(cls)]\n",
        "                    safety_info = self.safety_classes.get(\n",
        "                        class_name,\n",
        "                        {'type': 'unknown', 'color': (128, 128, 128), 'alpha': 0.5}\n",
        "                    )\n",
        "\n",
        "                    if i < len(masks):\n",
        "                        mask = masks[i]\n",
        "                        # Ensure mask points are valid\n",
        "                        if mask is not None and len(mask) > 0:\n",
        "                            # Convert mask points to integer array\n",
        "                            mask_points = np.array(mask, dtype=np.int32)\n",
        "\n",
        "                            # Ensure mask points are properly shaped\n",
        "                            if mask_points.shape[0] > 0:\n",
        "                                if class_name == 'sidewalk':\n",
        "                                    # Fill sidewalk area\n",
        "                                    cv2.fillPoly(safety_overlay, [mask_points], safety_info['color'])\n",
        "                                    # Draw outline\n",
        "                                    cv2.polylines(processed, [mask_points], True, safety_info['color'], 2)\n",
        "                                    # Add label\n",
        "                                    x_min, y_min = mask_points.min(axis=0)\n",
        "                                    cv2.putText(processed, \"SAFE ZONE\",\n",
        "                                              (int(x_min), int(y_min) - 10),\n",
        "                                              cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n",
        "                                              safety_info['color'], 2)\n",
        "                                else:\n",
        "                                    # Handle other objects\n",
        "                                    cv2.fillPoly(object_overlay, [mask_points], safety_info['color'])\n",
        "\n",
        "                                    # Add tracking ID and label\n",
        "                                    if track_ids is not None:\n",
        "                                        track_id = int(track_ids[i])\n",
        "                                        label = f\"ID {track_id} - {class_name}\"\n",
        "                                        x_min, y_min = mask_points.min(axis=0)\n",
        "                                        cv2.putText(processed, label,\n",
        "                                                  (int(x_min), int(y_min) - 10),\n",
        "                                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
        "                                                  safety_info['color'], 2)\n",
        "\n",
        "                # Blend overlays\n",
        "                processed = cv2.addWeighted(processed, 1, safety_overlay, 0.3, 0)\n",
        "                processed = cv2.addWeighted(processed, 1, object_overlay, 0.5, 0)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error processing frame: {str(e)}\")\n",
        "                return frame  # Return original frame if processing fails\n",
        "\n",
        "        return processed\n",
        "\n",
        "# Create output directory\n",
        "import os\n",
        "os.makedirs(\"safety_processed_frames\", exist_ok=True)\n",
        "\n",
        "# Initialize detector\n",
        "detector = SafetyZoneDetector()\n",
        "\n",
        "# Process frames\n",
        "for frame_file in sorted(os.listdir(\"frames\")):\n",
        "    frame_path = os.path.join(\"frames\", frame_file)\n",
        "    frame = cv2.imread(frame_path)\n",
        "\n",
        "    processed_frame = detector.process_frame(frame)\n",
        "\n",
        "    output_path = os.path.join(\"safety_processed_frames\", frame_file)\n",
        "    cv2.imwrite(output_path, processed_frame)\n",
        "    print(f\"✅ Processed: {frame_file}\")\n",
        "\n",
        "print(\"✅ Completed safety processing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aHoLcKhjGnm",
        "outputId": "9df4ecea-e3b2-4a4c-a48a-3ebb38e28be8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 4 persons, 1 train, 185.0ms\n",
            "Speed: 5.4ms preprocess, 185.0ms inference, 32.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5160.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 177.5ms\n",
            "Speed: 5.0ms preprocess, 177.5ms inference, 20.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5190.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 172.5ms\n",
            "Speed: 4.5ms preprocess, 172.5ms inference, 21.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5220.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 173.3ms\n",
            "Speed: 5.6ms preprocess, 173.3ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5250.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 176.8ms\n",
            "Speed: 4.4ms preprocess, 176.8ms inference, 23.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5280.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 188.7ms\n",
            "Speed: 5.3ms preprocess, 188.7ms inference, 27.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5310.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 bus, 258.8ms\n",
            "Speed: 7.1ms preprocess, 258.8ms inference, 32.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5340.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 bus, 1 parking meter, 268.8ms\n",
            "Speed: 6.9ms preprocess, 268.8ms inference, 33.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5370.jpg\n",
            "\n",
            "0: 384x640 2 persons, 307.4ms\n",
            "Speed: 5.7ms preprocess, 307.4ms inference, 62.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5400.jpg\n",
            "\n",
            "0: 384x640 1 person, 339.6ms\n",
            "Speed: 6.0ms preprocess, 339.6ms inference, 41.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5430.jpg\n",
            "\n",
            "0: 384x640 1 person, 322.1ms\n",
            "Speed: 4.4ms preprocess, 322.1ms inference, 35.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5460.jpg\n",
            "\n",
            "0: 384x640 1 person, 285.7ms\n",
            "Speed: 4.3ms preprocess, 285.7ms inference, 19.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5490.jpg\n",
            "\n",
            "0: 384x640 1 person, 284.2ms\n",
            "Speed: 4.5ms preprocess, 284.2ms inference, 22.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5520.jpg\n",
            "\n",
            "0: 384x640 1 person, 296.2ms\n",
            "Speed: 9.0ms preprocess, 296.2ms inference, 32.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5550.jpg\n",
            "\n",
            "0: 384x640 1 person, 199.6ms\n",
            "Speed: 4.6ms preprocess, 199.6ms inference, 19.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5580.jpg\n",
            "\n",
            "0: 384x640 1 person, 180.8ms\n",
            "Speed: 3.8ms preprocess, 180.8ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5610.jpg\n",
            "\n",
            "0: 384x640 1 person, 2 cars, 173.1ms\n",
            "Speed: 4.5ms preprocess, 173.1ms inference, 20.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5640.jpg\n",
            "\n",
            "0: 384x640 1 person, 2 cars, 172.8ms\n",
            "Speed: 6.2ms preprocess, 172.8ms inference, 24.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5670.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 178.7ms\n",
            "Speed: 4.9ms preprocess, 178.7ms inference, 42.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5700.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 429.6ms\n",
            "Speed: 4.2ms preprocess, 429.6ms inference, 49.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5730.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 322.7ms\n",
            "Speed: 4.7ms preprocess, 322.7ms inference, 58.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5760.jpg\n",
            "\n",
            "0: 384x640 1 person, 281.3ms\n",
            "Speed: 4.8ms preprocess, 281.3ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5790.jpg\n",
            "\n",
            "0: 384x640 2 persons, 175.0ms\n",
            "Speed: 4.6ms preprocess, 175.0ms inference, 12.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5820.jpg\n",
            "\n",
            "0: 384x640 2 persons, 173.6ms\n",
            "Speed: 4.4ms preprocess, 173.6ms inference, 15.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5850.jpg\n",
            "\n",
            "0: 384x640 1 person, 183.1ms\n",
            "Speed: 4.4ms preprocess, 183.1ms inference, 11.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5880.jpg\n",
            "\n",
            "0: 384x640 1 person, 177.6ms\n",
            "Speed: 4.4ms preprocess, 177.6ms inference, 15.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5910.jpg\n",
            "\n",
            "0: 384x640 5 persons, 1 potted plant, 179.5ms\n",
            "Speed: 4.8ms preprocess, 179.5ms inference, 13.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5940.jpg\n",
            "\n",
            "0: 384x640 1 person, 177.1ms\n",
            "Speed: 4.8ms preprocess, 177.1ms inference, 14.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5970.jpg\n",
            "\n",
            "0: 384x640 1 person, 189.6ms\n",
            "Speed: 4.3ms preprocess, 189.6ms inference, 23.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6000.jpg\n",
            "\n",
            "0: 384x640 2 persons, 175.4ms\n",
            "Speed: 4.7ms preprocess, 175.4ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6030.jpg\n",
            "\n",
            "0: 384x640 2 persons, 197.7ms\n",
            "Speed: 5.5ms preprocess, 197.7ms inference, 20.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6060.jpg\n",
            "\n",
            "0: 384x640 2 persons, 170.5ms\n",
            "Speed: 4.7ms preprocess, 170.5ms inference, 27.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6090.jpg\n",
            "\n",
            "0: 384x640 2 persons, 179.6ms\n",
            "Speed: 6.7ms preprocess, 179.6ms inference, 60.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6120.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 potted plant, 274.6ms\n",
            "Speed: 4.5ms preprocess, 274.6ms inference, 34.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6150.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 potted plant, 268.5ms\n",
            "Speed: 9.3ms preprocess, 268.5ms inference, 58.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6180.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 potted plant, 283.4ms\n",
            "Speed: 5.4ms preprocess, 283.4ms inference, 50.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6210.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 potted plant, 313.1ms\n",
            "Speed: 4.5ms preprocess, 313.1ms inference, 61.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6240.jpg\n",
            "\n",
            "0: 384x640 1 person, 176.8ms\n",
            "Speed: 4.4ms preprocess, 176.8ms inference, 33.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6270.jpg\n",
            "\n",
            "0: 384x640 3 persons, 180.3ms\n",
            "Speed: 5.4ms preprocess, 180.3ms inference, 47.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6300.jpg\n",
            "\n",
            "0: 384x640 1 person, 177.1ms\n",
            "Speed: 4.9ms preprocess, 177.1ms inference, 49.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6330.jpg\n",
            "\n",
            "0: 384x640 10 persons, 2 benchs, 1 umbrella, 1 potted plant, 186.0ms\n",
            "Speed: 5.2ms preprocess, 186.0ms inference, 28.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6360.jpg\n",
            "\n",
            "0: 384x640 1 person, 169.2ms\n",
            "Speed: 4.3ms preprocess, 169.2ms inference, 33.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6390.jpg\n",
            "\n",
            "0: 384x640 2 persons, 183.5ms\n",
            "Speed: 4.4ms preprocess, 183.5ms inference, 34.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6420.jpg\n",
            "\n",
            "0: 384x640 2 persons, 184.2ms\n",
            "Speed: 4.9ms preprocess, 184.2ms inference, 32.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6450.jpg\n",
            "\n",
            "0: 384x640 2 persons, 180.5ms\n",
            "Speed: 5.3ms preprocess, 180.5ms inference, 17.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6480.jpg\n",
            "\n",
            "0: 384x640 3 persons, 171.0ms\n",
            "Speed: 4.8ms preprocess, 171.0ms inference, 27.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6510.jpg\n",
            "\n",
            "0: 384x640 1 person, 177.3ms\n",
            "Speed: 5.1ms preprocess, 177.3ms inference, 15.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6540.jpg\n",
            "\n",
            "0: 384x640 1 person, 182.1ms\n",
            "Speed: 4.4ms preprocess, 182.1ms inference, 29.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6570.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 car, 1 traffic light, 1 parking meter, 4 potted plants, 180.4ms\n",
            "Speed: 4.3ms preprocess, 180.4ms inference, 23.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6600.jpg\n",
            "\n",
            "0: 384x640 8 persons, 1 parking meter, 2 potted plants, 187.0ms\n",
            "Speed: 4.9ms preprocess, 187.0ms inference, 24.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6630.jpg\n",
            "\n",
            "0: 384x640 7 persons, 1 parking meter, 187.1ms\n",
            "Speed: 4.3ms preprocess, 187.1ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6660.jpg\n",
            "\n",
            "0: 384x640 1 parking meter, 180.2ms\n",
            "Speed: 4.6ms preprocess, 180.2ms inference, 16.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6690.jpg\n",
            "\n",
            "0: 384x640 1 parking meter, 184.6ms\n",
            "Speed: 6.0ms preprocess, 184.6ms inference, 19.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6720.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 179.4ms\n",
            "Speed: 4.3ms preprocess, 179.4ms inference, 20.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6750.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 189.0ms\n",
            "Speed: 4.6ms preprocess, 189.0ms inference, 17.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6780.jpg\n",
            "\n",
            "0: 384x640 1 person, 178.7ms\n",
            "Speed: 4.3ms preprocess, 178.7ms inference, 27.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6810.jpg\n",
            "\n",
            "0: 384x640 1 person, 180.4ms\n",
            "Speed: 4.5ms preprocess, 180.4ms inference, 21.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6840.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 potted plant, 281.9ms\n",
            "Speed: 4.5ms preprocess, 281.9ms inference, 28.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6870.jpg\n",
            "\n",
            "0: 384x640 1 person, 287.3ms\n",
            "Speed: 4.5ms preprocess, 287.3ms inference, 23.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6900.jpg\n",
            "\n",
            "0: 384x640 1 person, 268.3ms\n",
            "Speed: 4.4ms preprocess, 268.3ms inference, 38.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6930.jpg\n",
            "\n",
            "0: 384x640 1 person, 281.3ms\n",
            "Speed: 4.9ms preprocess, 281.3ms inference, 35.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6960.jpg\n",
            "\n",
            "0: 384x640 1 person, 215.9ms\n",
            "Speed: 4.8ms preprocess, 215.9ms inference, 12.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6990.jpg\n",
            "\n",
            "0: 384x640 1 person, 187.2ms\n",
            "Speed: 4.9ms preprocess, 187.2ms inference, 10.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7020.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 potted plant, 177.9ms\n",
            "Speed: 5.1ms preprocess, 177.9ms inference, 36.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7050.jpg\n",
            "\n",
            "0: 384x640 1 person, 179.6ms\n",
            "Speed: 6.5ms preprocess, 179.6ms inference, 21.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7080.jpg\n",
            "\n",
            "0: 384x640 1 person, 180.6ms\n",
            "Speed: 4.2ms preprocess, 180.6ms inference, 23.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7110.jpg\n",
            "\n",
            "0: 384x640 1 person, 179.5ms\n",
            "Speed: 5.6ms preprocess, 179.5ms inference, 25.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7140.jpg\n",
            "\n",
            "0: 384x640 1 person, 171.9ms\n",
            "Speed: 4.4ms preprocess, 171.9ms inference, 25.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7170.jpg\n",
            "\n",
            "0: 384x640 1 person, 180.1ms\n",
            "Speed: 4.7ms preprocess, 180.1ms inference, 36.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7200.jpg\n",
            "\n",
            "0: 384x640 16 persons, 1 airplane, 1 potted plant, 174.2ms\n",
            "Speed: 4.5ms preprocess, 174.2ms inference, 37.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7230.jpg\n",
            "\n",
            "0: 384x640 1 person, 190.2ms\n",
            "Speed: 4.3ms preprocess, 190.2ms inference, 32.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7260.jpg\n",
            "\n",
            "0: 384x640 16 persons, 176.2ms\n",
            "Speed: 4.8ms preprocess, 176.2ms inference, 34.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7290.jpg\n",
            "\n",
            "0: 384x640 12 persons, 2 potted plants, 187.4ms\n",
            "Speed: 4.6ms preprocess, 187.4ms inference, 28.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7320.jpg\n",
            "\n",
            "0: 384x640 13 persons, 1 car, 2 benchs, 2 potted plants, 176.9ms\n",
            "Speed: 5.3ms preprocess, 176.9ms inference, 39.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7350.jpg\n",
            "\n",
            "0: 384x640 12 persons, 1 car, 1 potted plant, 185.3ms\n",
            "Speed: 4.4ms preprocess, 185.3ms inference, 31.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7380.jpg\n",
            "\n",
            "0: 384x640 11 persons, 1 car, 3 benchs, 2 potted plants, 178.2ms\n",
            "Speed: 5.4ms preprocess, 178.2ms inference, 41.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7410.jpg\n",
            "\n",
            "0: 384x640 1 person, 181.6ms\n",
            "Speed: 4.4ms preprocess, 181.6ms inference, 27.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7440.jpg\n",
            "\n",
            "0: 384x640 2 persons, 180.0ms\n",
            "Speed: 4.7ms preprocess, 180.0ms inference, 25.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7470.jpg\n",
            "\n",
            "0: 384x640 2 persons, 181.3ms\n",
            "Speed: 4.4ms preprocess, 181.3ms inference, 22.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7500.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 177.0ms\n",
            "Speed: 5.5ms preprocess, 177.0ms inference, 32.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7530.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 189.7ms\n",
            "Speed: 5.1ms preprocess, 189.7ms inference, 43.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7560.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 176.0ms\n",
            "Speed: 5.4ms preprocess, 176.0ms inference, 76.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7590.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 273.8ms\n",
            "Speed: 4.6ms preprocess, 273.8ms inference, 41.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7620.jpg\n",
            "\n",
            "0: 384x640 2 persons, 3 cars, 1 parking meter, 265.9ms\n",
            "Speed: 5.5ms preprocess, 265.9ms inference, 27.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7650.jpg\n",
            "\n",
            "0: 384x640 1 car, 270.7ms\n",
            "Speed: 4.7ms preprocess, 270.7ms inference, 35.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7680.jpg\n",
            "\n",
            "0: 384x640 1 car, 287.4ms\n",
            "Speed: 4.5ms preprocess, 287.4ms inference, 57.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7710.jpg\n",
            "\n",
            "0: 384x640 5 persons, 9 cars, 188.4ms\n",
            "Speed: 4.8ms preprocess, 188.4ms inference, 29.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7740.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 177.7ms\n",
            "Speed: 3.8ms preprocess, 177.7ms inference, 36.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7770.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 179.8ms\n",
            "Speed: 4.3ms preprocess, 179.8ms inference, 30.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7800.jpg\n",
            "\n",
            "0: 384x640 1 car, 177.9ms\n",
            "Speed: 6.7ms preprocess, 177.9ms inference, 21.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7830.jpg\n",
            "\n",
            "0: 384x640 2 cars, 188.1ms\n",
            "Speed: 4.3ms preprocess, 188.1ms inference, 37.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7860.jpg\n",
            "\n",
            "0: 384x640 1 car, 178.2ms\n",
            "Speed: 5.4ms preprocess, 178.2ms inference, 17.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7890.jpg\n",
            "\n",
            "0: 384x640 1 person, 5 cars, 1 truck, 178.4ms\n",
            "Speed: 4.9ms preprocess, 178.4ms inference, 18.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7920.jpg\n",
            "\n",
            "0: 384x640 1 car, 172.6ms\n",
            "Speed: 4.3ms preprocess, 172.6ms inference, 17.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7950.jpg\n",
            "\n",
            "0: 384x640 1 car, 182.6ms\n",
            "Speed: 5.5ms preprocess, 182.6ms inference, 27.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7980.jpg\n",
            "✅ Completed safety processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process frames according to safety analysis\n"
      ],
      "metadata": {
        "id": "2vHBXEz_jIET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "os.makedirs(\"safety_processed_frames\", exist_ok=True)\n",
        "detector = SafetyZoneDetector()\n",
        "\n",
        "# Process each frame\n",
        "frames_dir = \"frames\"\n",
        "frame_files = sorted(os.listdir(frames_dir))\n",
        "for frame_file in frame_files:\n",
        "    # Read frame\n",
        "    frame_path = os.path.join(frames_dir, frame_file)\n",
        "    frame = cv2.imread(frame_path)\n",
        "\n",
        "    # Process with safety detection\n",
        "    processed_frame = detector.process_frame(frame)\n",
        "\n",
        "    # Save processed frame\n",
        "    output_path = os.path.join(\"safety_processed_frames\", frame_file)\n",
        "    cv2.imwrite(output_path, processed_frame)\n",
        "    print(f\"✅ Processed: {frame_file}\")\n",
        "\n",
        "print(\"✅ Completed safety processing\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikXlQalujLDm",
        "outputId": "e09f316f-f49d-4dee-9591-fc9affebaff9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 4 persons, 1 train, 200.3ms\n",
            "Speed: 3.9ms preprocess, 200.3ms inference, 28.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5160.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 178.7ms\n",
            "Speed: 5.9ms preprocess, 178.7ms inference, 19.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5190.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 176.0ms\n",
            "Speed: 6.1ms preprocess, 176.0ms inference, 19.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5220.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 181.8ms\n",
            "Speed: 4.7ms preprocess, 181.8ms inference, 17.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5250.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 178.6ms\n",
            "Speed: 4.6ms preprocess, 178.6ms inference, 24.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5280.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 train, 182.3ms\n",
            "Speed: 5.4ms preprocess, 182.3ms inference, 26.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5310.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 bus, 184.4ms\n",
            "Speed: 3.8ms preprocess, 184.4ms inference, 19.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5340.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 bus, 1 parking meter, 175.4ms\n",
            "Speed: 5.2ms preprocess, 175.4ms inference, 19.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5370.jpg\n",
            "\n",
            "0: 384x640 2 persons, 487.0ms\n",
            "Speed: 15.0ms preprocess, 487.0ms inference, 49.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5400.jpg\n",
            "\n",
            "0: 384x640 1 person, 281.8ms\n",
            "Speed: 4.5ms preprocess, 281.8ms inference, 34.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5430.jpg\n",
            "\n",
            "0: 384x640 1 person, 272.6ms\n",
            "Speed: 4.4ms preprocess, 272.6ms inference, 34.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5460.jpg\n",
            "\n",
            "0: 384x640 1 person, 263.4ms\n",
            "Speed: 4.5ms preprocess, 263.4ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5490.jpg\n",
            "\n",
            "0: 384x640 1 person, 179.3ms\n",
            "Speed: 5.4ms preprocess, 179.3ms inference, 13.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5520.jpg\n",
            "\n",
            "0: 384x640 1 person, 180.8ms\n",
            "Speed: 6.0ms preprocess, 180.8ms inference, 19.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5550.jpg\n",
            "\n",
            "0: 384x640 1 person, 180.4ms\n",
            "Speed: 4.7ms preprocess, 180.4ms inference, 19.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5580.jpg\n",
            "\n",
            "0: 384x640 1 person, 179.7ms\n",
            "Speed: 5.5ms preprocess, 179.7ms inference, 22.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5610.jpg\n",
            "\n",
            "0: 384x640 1 person, 2 cars, 173.7ms\n",
            "Speed: 4.3ms preprocess, 173.7ms inference, 21.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5640.jpg\n",
            "\n",
            "0: 384x640 1 person, 2 cars, 187.4ms\n",
            "Speed: 4.3ms preprocess, 187.4ms inference, 23.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5670.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 177.4ms\n",
            "Speed: 4.7ms preprocess, 177.4ms inference, 27.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5700.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 192.0ms\n",
            "Speed: 4.6ms preprocess, 192.0ms inference, 26.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5730.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 179.5ms\n",
            "Speed: 4.8ms preprocess, 179.5ms inference, 15.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5760.jpg\n",
            "\n",
            "0: 384x640 1 person, 187.0ms\n",
            "Speed: 4.4ms preprocess, 187.0ms inference, 16.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5790.jpg\n",
            "\n",
            "0: 384x640 2 persons, 176.1ms\n",
            "Speed: 4.9ms preprocess, 176.1ms inference, 9.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5820.jpg\n",
            "\n",
            "0: 384x640 2 persons, 185.2ms\n",
            "Speed: 6.9ms preprocess, 185.2ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5850.jpg\n",
            "\n",
            "0: 384x640 1 person, 182.8ms\n",
            "Speed: 4.4ms preprocess, 182.8ms inference, 11.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5880.jpg\n",
            "\n",
            "0: 384x640 1 person, 202.5ms\n",
            "Speed: 9.5ms preprocess, 202.5ms inference, 16.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5910.jpg\n",
            "\n",
            "0: 384x640 5 persons, 1 potted plant, 192.1ms\n",
            "Speed: 4.9ms preprocess, 192.1ms inference, 13.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5940.jpg\n",
            "\n",
            "0: 384x640 1 person, 185.7ms\n",
            "Speed: 5.1ms preprocess, 185.7ms inference, 13.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_5970.jpg\n",
            "\n",
            "0: 384x640 1 person, 184.9ms\n",
            "Speed: 4.5ms preprocess, 184.9ms inference, 29.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6000.jpg\n",
            "\n",
            "0: 384x640 2 persons, 185.0ms\n",
            "Speed: 4.3ms preprocess, 185.0ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6030.jpg\n",
            "\n",
            "0: 384x640 2 persons, 186.6ms\n",
            "Speed: 4.8ms preprocess, 186.6ms inference, 20.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6060.jpg\n",
            "\n",
            "0: 384x640 2 persons, 192.5ms\n",
            "Speed: 4.9ms preprocess, 192.5ms inference, 27.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6090.jpg\n",
            "\n",
            "0: 384x640 2 persons, 274.0ms\n",
            "Speed: 6.4ms preprocess, 274.0ms inference, 53.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6120.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 potted plant, 262.5ms\n",
            "Speed: 6.9ms preprocess, 262.5ms inference, 36.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6150.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 potted plant, 280.1ms\n",
            "Speed: 4.6ms preprocess, 280.1ms inference, 48.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6180.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 potted plant, 283.6ms\n",
            "Speed: 5.2ms preprocess, 283.6ms inference, 49.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6210.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 potted plant, 201.9ms\n",
            "Speed: 5.0ms preprocess, 201.9ms inference, 46.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6240.jpg\n",
            "\n",
            "0: 384x640 1 person, 187.5ms\n",
            "Speed: 5.2ms preprocess, 187.5ms inference, 39.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6270.jpg\n",
            "\n",
            "0: 384x640 3 persons, 176.1ms\n",
            "Speed: 3.5ms preprocess, 176.1ms inference, 40.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6300.jpg\n",
            "\n",
            "0: 384x640 1 person, 179.6ms\n",
            "Speed: 6.7ms preprocess, 179.6ms inference, 42.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6330.jpg\n",
            "\n",
            "0: 384x640 10 persons, 2 benchs, 1 umbrella, 1 potted plant, 180.4ms\n",
            "Speed: 4.3ms preprocess, 180.4ms inference, 40.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6360.jpg\n",
            "\n",
            "0: 384x640 1 person, 174.0ms\n",
            "Speed: 4.7ms preprocess, 174.0ms inference, 32.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6390.jpg\n",
            "\n",
            "0: 384x640 2 persons, 177.5ms\n",
            "Speed: 4.2ms preprocess, 177.5ms inference, 52.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6420.jpg\n",
            "\n",
            "0: 384x640 2 persons, 175.2ms\n",
            "Speed: 4.6ms preprocess, 175.2ms inference, 39.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6450.jpg\n",
            "\n",
            "0: 384x640 2 persons, 193.4ms\n",
            "Speed: 4.7ms preprocess, 193.4ms inference, 18.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6480.jpg\n",
            "\n",
            "0: 384x640 3 persons, 173.3ms\n",
            "Speed: 4.6ms preprocess, 173.3ms inference, 27.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6510.jpg\n",
            "\n",
            "0: 384x640 1 person, 188.5ms\n",
            "Speed: 5.2ms preprocess, 188.5ms inference, 15.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6540.jpg\n",
            "\n",
            "0: 384x640 1 person, 178.0ms\n",
            "Speed: 5.1ms preprocess, 178.0ms inference, 28.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6570.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 car, 1 traffic light, 1 parking meter, 4 potted plants, 188.7ms\n",
            "Speed: 4.4ms preprocess, 188.7ms inference, 23.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6600.jpg\n",
            "\n",
            "0: 384x640 8 persons, 1 parking meter, 2 potted plants, 178.2ms\n",
            "Speed: 4.4ms preprocess, 178.2ms inference, 23.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6630.jpg\n",
            "\n",
            "0: 384x640 7 persons, 1 parking meter, 187.2ms\n",
            "Speed: 4.3ms preprocess, 187.2ms inference, 17.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6660.jpg\n",
            "\n",
            "0: 384x640 1 parking meter, 172.5ms\n",
            "Speed: 4.2ms preprocess, 172.5ms inference, 16.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6690.jpg\n",
            "\n",
            "0: 384x640 1 parking meter, 173.9ms\n",
            "Speed: 4.3ms preprocess, 173.9ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6720.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 168.1ms\n",
            "Speed: 5.0ms preprocess, 168.1ms inference, 21.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6750.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 175.4ms\n",
            "Speed: 5.6ms preprocess, 175.4ms inference, 21.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6780.jpg\n",
            "\n",
            "0: 384x640 1 person, 176.3ms\n",
            "Speed: 4.5ms preprocess, 176.3ms inference, 28.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6810.jpg\n",
            "\n",
            "0: 384x640 1 person, 281.4ms\n",
            "Speed: 4.4ms preprocess, 281.4ms inference, 34.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6840.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 potted plant, 269.4ms\n",
            "Speed: 6.5ms preprocess, 269.4ms inference, 29.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6870.jpg\n",
            "\n",
            "0: 384x640 1 person, 286.8ms\n",
            "Speed: 4.7ms preprocess, 286.8ms inference, 22.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6900.jpg\n",
            "\n",
            "0: 384x640 1 person, 275.6ms\n",
            "Speed: 7.9ms preprocess, 275.6ms inference, 38.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6930.jpg\n",
            "\n",
            "0: 384x640 1 person, 211.9ms\n",
            "Speed: 5.5ms preprocess, 211.9ms inference, 21.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6960.jpg\n",
            "\n",
            "0: 384x640 1 person, 173.8ms\n",
            "Speed: 7.2ms preprocess, 173.8ms inference, 9.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_6990.jpg\n",
            "\n",
            "0: 384x640 1 person, 182.2ms\n",
            "Speed: 4.7ms preprocess, 182.2ms inference, 9.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7020.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 potted plant, 176.5ms\n",
            "Speed: 4.4ms preprocess, 176.5ms inference, 19.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7050.jpg\n",
            "\n",
            "0: 384x640 1 person, 182.1ms\n",
            "Speed: 4.9ms preprocess, 182.1ms inference, 21.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7080.jpg\n",
            "\n",
            "0: 384x640 1 person, 178.2ms\n",
            "Speed: 4.6ms preprocess, 178.2ms inference, 30.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7110.jpg\n",
            "\n",
            "0: 384x640 1 person, 177.6ms\n",
            "Speed: 4.2ms preprocess, 177.6ms inference, 28.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7140.jpg\n",
            "\n",
            "0: 384x640 1 person, 174.3ms\n",
            "Speed: 5.2ms preprocess, 174.3ms inference, 27.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7170.jpg\n",
            "\n",
            "0: 384x640 1 person, 179.4ms\n",
            "Speed: 4.1ms preprocess, 179.4ms inference, 36.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7200.jpg\n",
            "\n",
            "0: 384x640 16 persons, 1 airplane, 1 potted plant, 174.1ms\n",
            "Speed: 4.6ms preprocess, 174.1ms inference, 53.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7230.jpg\n",
            "\n",
            "0: 384x640 1 person, 178.2ms\n",
            "Speed: 4.6ms preprocess, 178.2ms inference, 33.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7260.jpg\n",
            "\n",
            "0: 384x640 16 persons, 171.6ms\n",
            "Speed: 4.7ms preprocess, 171.6ms inference, 34.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7290.jpg\n",
            "\n",
            "0: 384x640 12 persons, 2 potted plants, 181.0ms\n",
            "Speed: 4.6ms preprocess, 181.0ms inference, 29.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7320.jpg\n",
            "\n",
            "0: 384x640 13 persons, 1 car, 2 benchs, 2 potted plants, 170.0ms\n",
            "Speed: 4.4ms preprocess, 170.0ms inference, 37.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7350.jpg\n",
            "\n",
            "0: 384x640 12 persons, 1 car, 1 potted plant, 175.8ms\n",
            "Speed: 4.3ms preprocess, 175.8ms inference, 29.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7380.jpg\n",
            "\n",
            "0: 384x640 11 persons, 1 car, 3 benchs, 2 potted plants, 185.7ms\n",
            "Speed: 5.3ms preprocess, 185.7ms inference, 37.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7410.jpg\n",
            "\n",
            "0: 384x640 1 person, 185.8ms\n",
            "Speed: 4.3ms preprocess, 185.8ms inference, 27.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7440.jpg\n",
            "\n",
            "0: 384x640 2 persons, 184.5ms\n",
            "Speed: 4.2ms preprocess, 184.5ms inference, 25.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7470.jpg\n",
            "\n",
            "0: 384x640 2 persons, 179.9ms\n",
            "Speed: 4.7ms preprocess, 179.9ms inference, 21.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7500.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 183.8ms\n",
            "Speed: 4.9ms preprocess, 183.8ms inference, 29.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7530.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 221.7ms\n",
            "Speed: 4.6ms preprocess, 221.7ms inference, 63.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7560.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 284.5ms\n",
            "Speed: 4.2ms preprocess, 284.5ms inference, 77.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7590.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 269.3ms\n",
            "Speed: 4.5ms preprocess, 269.3ms inference, 48.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7620.jpg\n",
            "\n",
            "0: 384x640 2 persons, 3 cars, 1 parking meter, 283.8ms\n",
            "Speed: 4.2ms preprocess, 283.8ms inference, 23.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7650.jpg\n",
            "\n",
            "0: 384x640 1 car, 301.8ms\n",
            "Speed: 8.4ms preprocess, 301.8ms inference, 36.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7680.jpg\n",
            "\n",
            "0: 384x640 1 car, 171.5ms\n",
            "Speed: 4.7ms preprocess, 171.5ms inference, 34.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7710.jpg\n",
            "\n",
            "0: 384x640 5 persons, 9 cars, 182.1ms\n",
            "Speed: 4.4ms preprocess, 182.1ms inference, 35.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7740.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 175.1ms\n",
            "Speed: 4.2ms preprocess, 175.1ms inference, 34.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7770.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 180.9ms\n",
            "Speed: 5.1ms preprocess, 180.9ms inference, 33.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7800.jpg\n",
            "\n",
            "0: 384x640 1 car, 179.9ms\n",
            "Speed: 5.4ms preprocess, 179.9ms inference, 21.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7830.jpg\n",
            "\n",
            "0: 384x640 2 cars, 189.6ms\n",
            "Speed: 5.1ms preprocess, 189.6ms inference, 32.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7860.jpg\n",
            "\n",
            "0: 384x640 1 car, 174.9ms\n",
            "Speed: 4.3ms preprocess, 174.9ms inference, 17.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7890.jpg\n",
            "\n",
            "0: 384x640 1 person, 5 cars, 1 truck, 189.1ms\n",
            "Speed: 4.5ms preprocess, 189.1ms inference, 15.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7920.jpg\n",
            "\n",
            "0: 384x640 1 car, 179.3ms\n",
            "Speed: 4.2ms preprocess, 179.3ms inference, 17.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7950.jpg\n",
            "\n",
            "0: 384x640 1 car, 200.0ms\n",
            "Speed: 4.9ms preprocess, 200.0ms inference, 25.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "✅ Processed: frame_7980.jpg\n",
            "✅ Completed safety processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating new video with the implementation"
      ],
      "metadata": {
        "id": "opG1lME0jO-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "frames_dir = \"safety_processed_frames\"\n",
        "output_video = \"phenix_walk.mp4\"\n",
        "\n",
        "# Get frame properties\n",
        "frame_files = sorted(os.listdir(frames_dir))\n",
        "first_frame = cv2.imread(os.path.join(frames_dir, frame_files[0]))\n",
        "height, width = first_frame.shape[:2]\n",
        "\n",
        "# Create video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video, fourcc, 30, (width, height))\n",
        "\n",
        "# Add frames to video\n",
        "for frame_file in frame_files:\n",
        "    frame = cv2.imread(os.path.join(frames_dir, frame_file))\n",
        "    out.write(frame)\n",
        "\n",
        "out.release()\n",
        "print(f\"✅ Created video: {output_video}\")\n",
        "\n",
        "# Download video (in Colab)\n",
        "from google.colab import files\n",
        "files.download(output_video)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "vmRdab6vjRid",
        "outputId": "652976e2-6df5-4497-db30-2be8bad65e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created video: phenix_walk.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_553ae7e7-0e3e-42c5-95de-1ca3715b3fdd\", \"phenix_walk.mp4\", 52384330)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing from roboflow, establish data.yaml, and train"
      ],
      "metadata": {
        "id": "3L3Q3FDItAO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List current directory contents\n",
        "!ls\n",
        "\n",
        "\n",
        "# List contents again to see what directory was created\n",
        "!ls\n",
        "\n",
        "# Print the directory structure\n",
        "!tree\n",
        "\n",
        "# Now let's create the yaml with the correct path\n",
        "import os\n",
        "\n",
        "# Create data.yaml with the exact path\n",
        "data_yaml = \"\"\"\n",
        "path: /content/Phoenix Data\n",
        "train: train/images\n",
        "val: valid/images\n",
        "test: test/images\n",
        "\n",
        "nc: 1\n",
        "names: ['walkable-zone']\n",
        "\"\"\"\n",
        "\n",
        "# Save the YAML configuration\n",
        "with open(\"Phoenix Data/data.yaml\", \"w\") as f:  # Save it inside Phoenix Data directory\n",
        "    f.write(data_yaml)\n",
        "\n",
        "# Verify the paths exist\n",
        "print(\"\\nVerifying paths...\")\n",
        "base_dir = \"/content/Phoenix Data\"\n",
        "train_path = os.path.join(base_dir, \"train/images\")\n",
        "valid_path = os.path.join(base_dir, \"valid/images\")\n",
        "test_path = os.path.join(base_dir, \"test/images\")\n",
        "\n",
        "print(f\"Train path exists: {os.path.exists(train_path)}\")\n",
        "print(f\"Valid path exists: {os.path.exists(valid_path)}\")\n",
        "print(f\"Test path exists: {os.path.exists(test_path)}\")\n",
        "\n",
        "# If paths are verified, train the model\n",
        "if os.path.exists(train_path) and os.path.exists(valid_path):\n",
        "    model = YOLO('yolov8n-seg.pt')\n",
        "    results = model.train(\n",
        "        data='Phoenix Data/data.yaml',  # Updated path to data.yaml\n",
        "        epochs=50,\n",
        "        imgsz=640,\n",
        "        batch=16,\n",
        "        name='phoenix_walkable3'\n",
        "    )\n",
        "else:\n",
        "    print(\"Error: Some required directories are missing!\")"
      ],
      "metadata": {
        "id": "NZnd1YCJtBcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7d6da6c0-34ac-42a7-ba50-050a810a7aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " data.yaml  'Phoenix Data'     runs\t     videos\n",
            " __MACOSX    phoenixdata.zip   sample_data   yolov8n-seg.pt\n",
            " data.yaml  'Phoenix Data'     runs\t     videos\n",
            " __MACOSX    phoenixdata.zip   sample_data   yolov8n-seg.pt\n",
            "/bin/bash: line 1: tree: command not found\n",
            "\n",
            "Verifying paths...\n",
            "Train path exists: True\n",
            "Valid path exists: True\n",
            "Test path exists: True\n",
            "Ultralytics 8.3.73 🚀 Python-3.11.11 torch-2.5.1+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=Phoenix Data/data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=phoenix_walkable35, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/segment/phoenix_walkable35\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1   1004275  ultralytics.nn.modules.head.Segment          [1, 32, 64, [64, 128, 256]]   \n",
            "YOLOv8n-seg summary: 261 layers, 3,263,811 parameters, 3,263,795 gradients, 12.1 GFLOPs\n",
            "\n",
            "Transferred 381/417 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/segment/phoenix_walkable35', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Phoenix Data/train/labels... 37 images, 0 backgrounds, 0 corrupt: 100%|██████████| 37/37 [00:00<00:00, 468.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Phoenix Data/train/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Phoenix Data/valid/labels... 3 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3/3 [00:00<00:00, 1213.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Phoenix Data/valid/labels.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/segment/phoenix_walkable35/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/segment/phoenix_walkable35\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/50         0G      1.276      4.656       3.02      1.649          8        640: 100%|██████████| 3/3 [00:58<00:00, 19.41s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3    0.00333          1      0.111     0.0638    0.00333          1      0.111     0.0539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/50         0G      0.891      2.268      2.588      1.339         10        640: 100%|██████████| 3/3 [00:48<00:00, 16.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3    0.00333          1      0.995      0.701    0.00333          1      0.995       0.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/50         0G     0.7733      1.689      2.252      1.209         13        640: 100%|██████████| 3/3 [00:47<00:00, 15.84s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3    0.00333          1      0.995      0.735    0.00333          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/50         0G     0.8609      1.455      1.898      1.271          9        640: 100%|██████████| 3/3 [00:45<00:00, 15.08s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3    0.00333          1      0.995       0.78    0.00333          1      0.995      0.813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       5/50         0G     0.8233      1.421      1.636      1.271         16        640: 100%|██████████| 3/3 [00:44<00:00, 14.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3    0.00333          1      0.995      0.654    0.00333          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       6/50         0G     0.8295      1.249      1.397       1.27         13        640: 100%|██████████| 3/3 [00:43<00:00, 14.61s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3    0.00333          1      0.995       0.83    0.00333          1      0.995      0.929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       7/50         0G     0.7156      1.165      1.343      1.139         10        640: 100%|██████████| 3/3 [00:46<00:00, 15.48s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3    0.00333          1      0.995      0.929    0.00333          1      0.995       0.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       8/50         0G     0.7113       1.19      1.367       1.18         10        640: 100%|██████████| 3/3 [00:45<00:00, 15.30s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.217          1      0.995      0.912      0.217          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       9/50         0G     0.7701      1.026       1.41      1.278          9        640: 100%|██████████| 3/3 [00:45<00:00, 15.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.926          1      0.995      0.895      0.926          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      10/50         0G     0.7693      1.028      1.354      1.238         11        640: 100%|██████████| 3/3 [00:49<00:00, 16.52s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.986          1      0.995      0.863      0.986          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      11/50         0G     0.5814      1.004      1.167      1.104         13        640: 100%|██████████| 3/3 [00:46<00:00, 15.61s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.856          1      0.995      0.929      0.856          1      0.995      0.962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      12/50         0G     0.7493      1.147      1.261      1.186          8        640: 100%|██████████| 3/3 [00:44<00:00, 14.91s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.824          1      0.995      0.896      0.824          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      13/50         0G     0.6846     0.9648      1.132      1.097         18        640: 100%|██████████| 3/3 [00:46<00:00, 15.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.898          1      0.995      0.857      0.898          1      0.995      0.763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      14/50         0G     0.7496     0.9589      1.263      1.181         10        640: 100%|██████████| 3/3 [00:45<00:00, 15.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.967          1      0.995      0.767      0.967          1      0.995       0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      15/50         0G     0.8011      1.086      1.162      1.255         13        640: 100%|██████████| 3/3 [00:45<00:00, 15.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.973          1      0.995      0.912      0.973          1      0.995      0.885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      16/50         0G     0.7247     0.8638      1.079      1.144         13        640: 100%|██████████| 3/3 [00:51<00:00, 17.03s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.984          1      0.995       0.84      0.984          1      0.995      0.796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      17/50         0G     0.7405      1.321      1.142      1.197         13        640: 100%|██████████| 3/3 [00:45<00:00, 15.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3          1      0.977      0.995      0.854          1      0.977      0.995      0.764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      18/50         0G     0.8284      1.179      1.299      1.243         14        640: 100%|██████████| 3/3 [00:45<00:00, 15.19s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.983          1      0.995      0.896      0.983          1      0.995      0.863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      19/50         0G     0.7404       0.98      1.084       1.11         15        640: 100%|██████████| 3/3 [00:44<00:00, 14.78s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.984          1      0.995      0.895      0.984          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      20/50         0G     0.6256      1.201       1.08      1.055         10        640: 100%|██████████| 3/3 [00:51<00:00, 17.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.985          1      0.995      0.895      0.985          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      21/50         0G     0.6723     0.9492      1.047      1.155          9        640: 100%|██████████| 3/3 [00:45<00:00, 15.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.985          1      0.995      0.895      0.985          1      0.995       0.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      22/50         0G     0.7645      1.156     0.9652      1.198         16        640: 100%|██████████| 3/3 [00:45<00:00, 15.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.981          1      0.995      0.852      0.981          1      0.995      0.852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      23/50         0G     0.7307      1.033       1.05      1.096         10        640: 100%|██████████| 3/3 [00:46<00:00, 15.48s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.976          1      0.995      0.885      0.976          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      24/50         0G      0.716      1.016          1      1.164         10        640: 100%|██████████| 3/3 [00:45<00:00, 15.13s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.949          1      0.995      0.664      0.949          1      0.995       0.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      25/50         0G     0.8454     0.9619      1.057      1.187         10        640: 100%|██████████| 3/3 [00:46<00:00, 15.38s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.938          1      0.995      0.808      0.938          1      0.995      0.808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      26/50         0G     0.7592      1.051       1.04       1.18         13        640: 100%|██████████| 3/3 [00:44<00:00, 14.92s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3       0.97          1      0.995       0.83       0.97          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      27/50         0G      0.725      1.084      1.034      1.143         11        640: 100%|██████████| 3/3 [00:45<00:00, 15.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3       0.98          1      0.995      0.764       0.98          1      0.995      0.863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      28/50         0G     0.8337      1.042      1.059      1.193         14        640: 100%|██████████| 3/3 [00:48<00:00, 16.05s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.978          1      0.995      0.857      0.978          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      29/50         0G     0.7285     0.9017     0.9092      1.162         12        640: 100%|██████████| 3/3 [00:45<00:00, 15.20s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.978          1      0.995      0.857      0.978          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      30/50         0G     0.6255     0.9809      1.033      1.115          9        640: 100%|██████████| 3/3 [00:44<00:00, 14.92s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.978          1      0.995      0.951      0.978          1      0.995      0.885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      31/50         0G     0.6901     0.9289     0.9466       1.11         14        640: 100%|██████████| 3/3 [00:44<00:00, 14.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.975          1      0.995      0.995      0.975          1      0.995      0.929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      32/50         0G     0.7746     0.9235      1.016        1.2         12        640: 100%|██████████| 3/3 [00:43<00:00, 14.64s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.981          1      0.995      0.995      0.981          1      0.995      0.929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      33/50         0G     0.7163     0.8677     0.8707      1.136         14        640: 100%|██████████| 3/3 [00:49<00:00, 16.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.981          1      0.995      0.995      0.981          1      0.995      0.929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      34/50         0G     0.7627      1.036     0.9773      1.161         12        640: 100%|██████████| 3/3 [00:47<00:00, 15.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.982          1      0.995      0.907      0.982          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      35/50         0G      0.729     0.9665     0.8773      1.078         12        640: 100%|██████████| 3/3 [00:45<00:00, 15.16s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.981          1      0.995      0.852      0.981          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      36/50         0G      0.782     0.9331     0.9097      1.217         14        640: 100%|██████████| 3/3 [00:46<00:00, 15.49s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.983          1      0.995       0.84      0.983          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      37/50         0G     0.6564     0.8565     0.8125      1.179         10        640: 100%|██████████| 3/3 [00:45<00:00, 15.25s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.983          1      0.995       0.84      0.983          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      38/50         0G     0.6463     0.8048     0.7838      1.129         16        640: 100%|██████████| 3/3 [00:45<00:00, 15.25s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.983          1      0.995      0.852      0.983          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      39/50         0G     0.6375     0.7766     0.8253      1.061         13        640: 100%|██████████| 3/3 [00:46<00:00, 15.49s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.983          1      0.995      0.885      0.983          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      40/50         0G     0.6394     0.9448     0.8433      1.061         12        640: 100%|██████████| 3/3 [00:44<00:00, 14.81s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.982          1      0.995      0.885      0.982          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      41/50         0G      0.558     0.9223      1.403      1.177          5        640: 100%|██████████| 3/3 [00:43<00:00, 14.60s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.982          1      0.995      0.885      0.982          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      42/50         0G     0.5969     0.7507      1.072      1.201          5        640: 100%|██████████| 3/3 [00:44<00:00, 14.71s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.982          1      0.995      0.885      0.982          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      43/50         0G      0.546     0.6395      1.118       1.17          5        640: 100%|██████████| 3/3 [00:44<00:00, 14.69s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.978          1      0.995       0.94      0.978          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      44/50         0G     0.5346     0.7027      1.039      1.142          6        640: 100%|██████████| 3/3 [00:43<00:00, 14.65s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.982          1      0.995       0.94      0.982          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      45/50         0G     0.5414     0.8298      1.196      1.222          5        640: 100%|██████████| 3/3 [00:43<00:00, 14.57s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.982          1      0.995       0.94      0.982          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      46/50         0G     0.4779     0.6469       1.03      1.133          6        640: 100%|██████████| 3/3 [00:46<00:00, 15.34s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.983          1      0.995       0.94      0.983          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      47/50         0G     0.5664     0.6707      1.047      1.213          6        640: 100%|██████████| 3/3 [00:43<00:00, 14.56s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.983          1      0.995       0.94      0.983          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      48/50         0G     0.3931     0.5779      1.006      1.026          5        640: 100%|██████████| 3/3 [00:43<00:00, 14.57s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.982          1      0.995       0.94      0.982          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      49/50         0G     0.4996      0.659      1.014      1.138          6        640: 100%|██████████| 3/3 [00:42<00:00, 14.32s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.982          1      0.995       0.94      0.982          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      50/50         0G     0.5251     0.7489      1.005      1.174          6        640: 100%|██████████| 3/3 [00:45<00:00, 15.31s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.983          1      0.995      0.907      0.983          1      0.995      0.895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "50 epochs completed in 0.675 hours.\n",
            "Optimizer stripped from runs/segment/phoenix_walkable35/weights/last.pt, 6.8MB\n",
            "Optimizer stripped from runs/segment/phoenix_walkable35/weights/best.pt, 6.8MB\n",
            "\n",
            "Validating runs/segment/phoenix_walkable35/weights/best.pt...\n",
            "Ultralytics 8.3.73 🚀 Python-3.11.11 torch-2.5.1+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n-seg summary (fused): 195 layers, 3,258,259 parameters, 0 gradients, 12.0 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all          3          3      0.981          1      0.995      0.995      0.981          1      0.995      0.929\n",
            "Speed: 2.7ms preprocess, 319.9ms inference, 0.0ms loss, 7.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns/segment/phoenix_walkable35\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model based on this fine-tuning"
      ],
      "metadata": {
        "id": "NWA9b1feuJy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your trained model weights\n",
        "model.save('walkable_model.pt')\n",
        "\n",
        "#download to google colab\n",
        "from google.colab import files\n",
        "files.download('walkable_model.pt')\n",
        "\n",
        "# For next time when you want to continue training:\n",
        "# model = YOLO('walkable_model.pt')  # Load your saved model instead of yolov8n-seg.pt\n",
        "# results = model.train(\n",
        "#   data='data.yaml',\n",
        "#   epochs=50,\n",
        "#   imgsz=640,\n",
        " #   batch=16,\n",
        "  # name='phoenix_walkable_continued',\n",
        " #   resume=True  # This tells it to continue from your saved weights\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ClxbNU9-uUv9",
        "outputId": "b85aa186-756c-4c12-cb39-6897d29f3a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a5e806c7-2f35-4fe5-8e36-d0543a771d0b\", \"walkable_model.pt\", 6806663)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model on the video"
      ],
      "metadata": {
        "id": "4tJAy-MvzQe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First download the video\n",
        "\n",
        "!pip uninstall -y torch torchvision ultralytics opencv-python\n",
        "\n",
        "!pip install ultralytics\n",
        "\n",
        "!pip install yt-dlp\n",
        "\n",
        "!pip install opencv-python\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import glob\n",
        "\n",
        "def draw_mask_safely(frame, mask, color, alpha=0.5):\n",
        "    \"\"\"Safely draw a mask on a frame with validation\"\"\"\n",
        "    try:\n",
        "        # Validate mask\n",
        "        if mask is None or len(mask) < 3:  # Need at least 3 points for a polygon\n",
        "            return frame\n",
        "\n",
        "        # Convert to correct format\n",
        "        mask_points = np.array(mask, dtype=np.int32)\n",
        "        if mask_points.shape[0] < 3:  # Double-check after conversion\n",
        "            return frame\n",
        "\n",
        "        # Create overlay\n",
        "        overlay = frame.copy()\n",
        "        cv2.fillPoly(overlay, [mask_points], color)\n",
        "        return cv2.addWeighted(frame, 1-alpha, overlay, alpha, 0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not draw mask: {e}\")\n",
        "        return frame\n",
        "\n",
        "os.makedirs(\"videos\", exist_ok=True)\n",
        "VIDEO_URL = \"https://www.youtube.com/watch?v=bZnjrRuBT_k\"\n",
        "!yt-dlp -o \"videos/Phoenix_Walk.%(ext)s\" {VIDEO_URL}\n",
        "\n",
        "# Load both models\n",
        "walkable_model = YOLO('walkable_model.pt')\n",
        "object_model = YOLO('yolov8n-seg.pt')\n",
        "\n",
        "# Set up video capture\n",
        "video_path = glob.glob(\"videos/Phoenix_Walk.*\")[0]\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Create output video writer\n",
        "output_path = 'processed_video.mp4'\n",
        "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "# Set time range (1:26 to 2:13)\n",
        "start_sec, end_sec = 86, 133\n",
        "start_frame = int(start_sec * fps)\n",
        "end_frame = int(end_sec * fps)\n",
        "\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened() and frame_count < (end_frame - start_frame):\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    processed_frame = frame.copy()\n",
        "\n",
        "    # 1. Detect walkable zones with your fine-tuned model\n",
        "    walkable_results = walkable_model.predict(frame, show=False)\n",
        "    if walkable_results[0].masks is not None:\n",
        "        for mask in walkable_results[0].masks.xy:\n",
        "            # Safely draw walkable zones\n",
        "            processed_frame = draw_mask_safely(\n",
        "                processed_frame,\n",
        "                mask,\n",
        "                color=(0, 255, 0),  # Green for walkable zones\n",
        "                alpha=0.3\n",
        "            )\n",
        "            # Add outline if mask is valid\n",
        "            if len(mask) >= 3:\n",
        "                cv2.polylines(processed_frame, [np.int32(mask)], True, (0, 255, 0), 2)\n",
        "\n",
        "    # 2. Detect and track objects with original model\n",
        "    object_results = object_model.track(frame, persist=True)[0]\n",
        "    if object_results.boxes is not None and object_results.masks is not None:\n",
        "        masks = object_results.masks.xy\n",
        "        track_ids = object_results.boxes.id.cpu().numpy() if object_results.boxes.id is not None else None\n",
        "        classes = object_results.boxes.cls.cpu().numpy()\n",
        "\n",
        "        for i, (mask, cls) in enumerate(zip(masks, classes)):\n",
        "            # Safely draw objects\n",
        "            processed_frame = draw_mask_safely(\n",
        "                processed_frame,\n",
        "                mask,\n",
        "                color=(255, 165, 0),  # Orange for objects\n",
        "                alpha=0.5\n",
        "            )\n",
        "\n",
        "            # Add tracking ID and label\n",
        "            if track_ids is not None and len(mask) > 0:\n",
        "                track_id = int(track_ids[i])\n",
        "                class_name = object_model.names[int(cls)]\n",
        "                label = f\"ID {track_id} - {class_name}\"\n",
        "                x_min, y_min = mask.min(axis=0)\n",
        "                cv2.putText(processed_frame, label,\n",
        "                          (int(x_min), int(y_min) - 10),\n",
        "                          cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                          0.6,\n",
        "                          (255, 255, 255),\n",
        "                          2)\n",
        "\n",
        "    out.write(processed_frame)\n",
        "    frame_count += 1\n",
        "    if frame_count % 30 == 0:\n",
        "        print(f\"Processed frame {frame_count}\")\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Download the processed video\n",
        "from google.colab import files\n",
        "files.download('processed_video.mp4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "44NuHhn8zSJ-",
        "outputId": "90c62fd1-e801-4fe7-fc25-6bc6cb64b20b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0\n",
            "Uninstalling torch-2.6.0:\n",
            "  Successfully uninstalled torch-2.6.0\n",
            "Found existing installation: torchvision 0.21.0\n",
            "Uninstalling torchvision-0.21.0:\n",
            "  Successfully uninstalled torchvision-0.21.0\n",
            "Found existing installation: ultralytics 8.3.75\n",
            "Uninstalling ultralytics-8.3.75:\n",
            "  Successfully uninstalled ultralytics-8.3.75\n",
            "Found existing installation: opencv-python 4.11.0.86\n",
            "Uninstalling opencv-python-4.11.0.86:\n",
            "  Successfully uninstalled opencv-python-4.11.0.86\n",
            "Collecting ultralytics\n",
            "  Using cached ultralytics-8.3.75-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
            "Collecting torch>=1.8.0 (from ultralytics)\n",
            "  Using cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision>=0.9.0 (from ultralytics)\n",
            "  Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Using cached ultralytics-8.3.75-py3-none-any.whl (914 kB)\n",
            "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "Using cached torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "Installing collected packages: opencv-python, torch, torchvision, ultralytics\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed opencv-python-4.11.0.86 torch-2.6.0 torchvision-0.21.0 ultralytics-8.3.75\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2",
                  "torch"
                ]
              },
              "id": "0c285a67eb6f48f191c275f338e35107"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.11/dist-packages (2025.1.26)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 324, in run\n",
            "    session = self.get_default_session(options)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/index_command.py\", line 71, in get_default_session\n",
            "    self._session = self.enter_context(self._build_session(options))\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/index_command.py\", line 100, in _build_session\n",
            "    session = PipSession(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/network/session.py\", line 344, in __init__\n",
            "    self.headers[\"User-Agent\"] = user_agent()\n",
            "                                 ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/network/session.py\", line 177, in user_agent\n",
            "    setuptools_dist = get_default_environment().get_distribution(\"setuptools\")\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 189, in get_distribution\n",
            "    return next(matches, None)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 184, in <genexpr>\n",
            "    matches = (\n",
            "              ^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/base.py\", line 612, in iter_all_distributions\n",
            "    for dist in self._iter_distributions():\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 176, in _iter_distributions\n",
            "    yield from finder.find(location)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 79, in find\n",
            "    for dist, info_location in self._find_impl(location):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_envs.py\", line 64, in _find_impl\n",
            "    raw_name = get_dist_name(dist)\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_compat.py\", line 52, in get_dist_name\n",
            "    name = cast(Any, dist).name\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 622, in name\n",
            "    return self.metadata['Name']\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 617, in metadata\n",
            "    return _adapters.Message(email.message_from_string(text))\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/__init__.py\", line 37, in message_from_string\n",
            "    return Parser(*args, **kws).parsestr(s)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/parser.py\", line 67, in parsestr\n",
            "    return self.parse(StringIO(text), headersonly=headersonly)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/parser.py\", line 56, in parse\n",
            "    feedparser.feed(data)\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 174, in feed\n",
            "    self._call_parse()\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 178, in _call_parse\n",
            "    self._parse()\n",
            "  File \"/usr/lib/python3.11/email/feedparser.py\", line 293, in _parsegen\n",
            "    if self._cur.get_content_maintype() == 'message':\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/message.py\", line 622, in get_content_maintype\n",
            "    ctype = self.get_content_type()\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/email/message.py\", line 592, in get_content_type\n",
            "    def get_content_type(self):\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1526, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch' has no attribute '__version__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ee1b4ecddc9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OMP_NUM_THREADS\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1\"\u001b[0m  \u001b[0;31m# default for reduced CPU utilization during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNAS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRTDETR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastSAM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYOLOWorld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mASSETS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSETTINGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_yolo\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mchecks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfastsam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastSAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNAS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrtdetr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRTDETR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/fastsam/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastSAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastSAMPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastSAMValidator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/fastsam/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastSAMPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTASK2DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_save_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHUB_WEB_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHUBTrainingSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/cfg/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from ultralytics.utils import (\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mASSETS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mDEFAULT_CFG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mARM64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"arm64\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aarch64\"\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# ARM64 booleans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mPYTHON_VERSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mTORCH_VERSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mTORCHVISION_VERSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# faster than importing torchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mIS_VSCODE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TERM_PROGRAM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vscode\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '__version__'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proximity Detector"
      ],
      "metadata": {
        "id": "rcp-sA4mkon5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "class ProximityDetector:\n",
        "    def __init__(self):\n",
        "        # Only need YOLO for object detection\n",
        "        self.walkable_model = YOLO('walkable_model.pt')  # For walkable areas\n",
        "        self.object_model = YOLO('yolov8n-seg.pt')\n",
        "        # Define immediate proximity zone (area right in front)\n",
        "        self.proximity_height = 0.4  # Bottom 40% of frame\n",
        "        self.proximity_width = 0.4   # Center 40% of frame width\n",
        "\n",
        "    def is_in_proximity(self, box, frame_shape):\n",
        "        \"\"\"Check if object is very close to robot's path\"\"\"\n",
        "        height, width = frame_shape[:2]\n",
        "\n",
        "        # Get box coordinates\n",
        "        x1, y1, x2, y2 = box\n",
        "        box_center_x = (x1 + x2) / 2\n",
        "        box_bottom = y2\n",
        "\n",
        "        # Define immediate front zone\n",
        "        zone_left = width * (0.5 - self.proximity_width/2)\n",
        "        zone_right = width * (0.5 + self.proximity_width/2)\n",
        "        zone_top = height * (1 - self.proximity_height)\n",
        "\n",
        "        # Check if object is in immediate front zone\n",
        "        if (zone_left < box_center_x < zone_right and box_bottom > zone_top):\n",
        "            relative_distance = (box_bottom - zone_top) / (height - zone_top)\n",
        "            return True, relative_distance\n",
        "        return False, 0\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        height, width = frame.shape[:2]\n",
        "        processed = frame.copy()\n",
        "\n",
        "        # Detect and track objects\n",
        "        results = self.model.track(frame, persist=True)[0]\n",
        "\n",
        "        if results.boxes is not None:\n",
        "            boxes = results.boxes.xyxy.cpu().numpy()\n",
        "            classes = results.boxes.cls.cpu().numpy()\n",
        "\n",
        "            # Track closest obstacle\n",
        "            closest_distance = 0\n",
        "            has_close_obstacle = False\n",
        "\n",
        "            for box, cls in zip(boxes, classes):\n",
        "                class_name = self.model.names[int(cls)]\n",
        "\n",
        "                # Only consider person, bicycle, car as obstacles\n",
        "                if class_name in ['person', 'bicycle', 'car']:\n",
        "                    in_proximity, distance = self.is_in_proximity(box, frame.shape)\n",
        "\n",
        "                    if in_proximity and distance > 0.3:  # Object significantly in front\n",
        "                        has_close_obstacle = True\n",
        "                        if distance > closest_distance:\n",
        "                            closest_distance = distance\n",
        "                            # Draw red box around closest obstacle\n",
        "                            cv2.rectangle(processed,\n",
        "                                        (int(box[0]), int(box[1])),\n",
        "                                        (int(box[2]), int(box[3])),\n",
        "                                        (0, 0, 255), 2)\n",
        "\n",
        "            # Add warning text if obstacle detected\n",
        "            if has_close_obstacle:\n",
        "                warning_text = \"Obstacle ahead, please move\"\n",
        "                text_size = cv2.getTextSize(warning_text, cv2.FONT_HERSHEY_SIMPLEX, 1.2, 3)[0]\n",
        "                text_x = (width - text_size[0]) // 2\n",
        "                cv2.putText(processed, warning_text,\n",
        "                          (text_x, height - 50),\n",
        "                          cv2.FONT_HERSHEY_SIMPLEX, 1.2,\n",
        "                          (0, 0, 255), 3)\n",
        "\n",
        "        return processed"
      ],
      "metadata": {
        "id": "t034KzvklQOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video"
      ],
      "metadata": {
        "id": "s4gcmykDnnSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Make a video processor\n",
        "detector = ProximityDetector()\n",
        "\n",
        "# Open your video\n",
        "cap = cv2.VideoCapture(\"processed_video_copy.mp4\")  # Use your video filename here\n",
        "if not cap.isOpened():\n",
        "    print(\"Could not open video\")\n",
        "    exit()\n",
        "\n",
        "# Create video writer\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "# Process each frame\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Process frame\n",
        "    processed_frame = detector.process_frame(frame)\n",
        "    out.write(processed_frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# Download the video\n",
        "from google.colab import files\n",
        "files.download('output.mp4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "VdrCexdi21tK",
        "outputId": "c0427e74-cf33-4209-c77f-c2b884b91199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not open video\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a91300f7-563b-4eba-a962-c128511d31e4\", \"output.mp4\", 616082454)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}